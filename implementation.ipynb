{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhayanesh/sentimentAnalysis-seq2seq/blob/main/implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VgA4_zZ1S0Z"
      },
      "source": [
        "**Sentiment Analysis:**\n",
        "\n",
        "IMDB movie review sentiment classification data set contains 25,000 reviews of popular movies in the training split and another 25,000 in the testing split. Each review is labeled as either positive or negative. The task here is to predict the label from a review.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFDcfnog1GOz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import keras\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Conv1D, MaxPooling1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT0FdFjaV4ln",
        "outputId": "fb443a3e-7dc3-4949-fcf7-5c9f9d03dd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9346k      0  0:00:08  0:00:08 --:--:-- 15.5M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB3QveaYV7Ic"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcv0teB2WNOi",
        "outputId": "6e883e96-ab7c-4ef8-edc2-552fa8bf27a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "seed = 7\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/train\", subset=\"training\", seed=seed, validation_split=0.2, batch_size=batch_size)\n",
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/train\", subset=\"validation\", seed=seed, validation_split=0.2, batch_size=batch_size)\n",
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqIuETPQXQRi"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  input_data_lower = tf.strings.lower(input_data)\n",
        "  input_data_no_html = tf.strings.regex_replace(input_data_lower, \"<br />\", \" \")\n",
        "  standardized_input = tf.strings.regex_replace(input_data_no_html, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "  return standardized_input\n",
        "\n",
        "sequence_length = 500\n",
        "vectorize_layer = TextVectorization(standardize=custom_standardization, max_tokens=30000, output_mode=\"int\", output_sequence_length=500)\n",
        "\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JguxuQerCcnD"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H-bnJpuf5U_",
        "outputId": "06298e96-3b21-48b5-9a44-13f6d3dde96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Detected 400000 Word Vectors***\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import io\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "def getEmbeddings(path):\n",
        "    emb_idx = {}\n",
        "    with zipfile.ZipFile(path, \"r\") as f:\n",
        "        with io.TextIOWrapper(f.open(\"glove.6B.100d.txt\"), encoding=\"utf-8\") as text_file:\n",
        "            for line in text_file:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                float_values = []\n",
        "                for val in values[1:]:\n",
        "                    float_val = float(val)\n",
        "                    float_values.append(float_val)\n",
        "\n",
        "                coefs = tf.constant(float_values)\n",
        "                emb_idx[word] = coefs\n",
        "    return emb_idx\n",
        "\n",
        "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "path = \"glove.6B.zip\"\n",
        "urllib.request.urlretrieve(url, path)\n",
        "embs_idx = getEmbeddings(path)\n",
        "print(\"***Detected %s Word Vectors***\" % len(embs_idx))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltrlrpmMOL_7",
        "outputId": "6e41a751-9f92-4a38-f7e5-20a39df2cf01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(next(iter(embs_idx.values())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb7ASttvAZOe",
        "outputId": "9d2b7167-4862-4a34-ad4c-b7ba23d2b5bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=float32, numpy=\n",
              "array([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
              "       -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
              "       -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
              "        2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
              "       -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
              "        7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
              "       -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
              "        4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
              "       -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
              "       -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
              "       -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
              "       -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
              "       -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
              "       -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
              "       -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
              "        5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
              "        8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
              "       -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
              "        5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
              "        1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "embs_idx[\"green\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw0wNHNdOz3g"
      },
      "outputs": [],
      "source": [
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "\n",
        "word_index = {}\n",
        "for idx, word in enumerate(vocabulary):\n",
        "    word_index[word] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCBifw_GyNgE"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(word_index) + 2\n",
        "embedding_dim = len(next(iter(embs_idx.values())))\n",
        "\n",
        "embedding_matrix = tf.Variable(tf.zeros((num_tokens, embedding_dim)), trainable=False)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embs_idx.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i].assign(embedding_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6LeUihYswgu",
        "outputId": "d65c9bd8-ac5e-4b35-dffc-ba9cce221ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 100)          3000200   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 496, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 128)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,196,041\n",
            "Trainable params: 3,196,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim, input_length=sequence_length,\n",
        "                    embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=True))\n",
        "\n",
        "l2_reg = 0.001\n",
        "\n",
        "model.add(Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT4CM0iaVII9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "689ac712-d813-4164-eedc-9b4b02586a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: True\n",
            "Num GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"GPU:\", tf.test.is_built_with_cuda())\n",
        "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ZQLjZ4cebL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, mode='max', verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f-Oc-eWDtRz",
        "outputId": "6e918aae-32d0-4f86-e33c-a407c7239b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 117s 167ms/step - loss: 0.7106 - accuracy: 0.5066 - val_loss: 0.6949 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.6949 - accuracy: 0.5082 - val_loss: 0.6937 - val_accuracy: 0.5090\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.6935 - accuracy: 0.5060 - val_loss: 0.6925 - val_accuracy: 0.5114\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.6913 - accuracy: 0.5130 - val_loss: 0.6908 - val_accuracy: 0.5162\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.6896 - accuracy: 0.5195 - val_loss: 0.6937 - val_accuracy: 0.5170\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.6788 - accuracy: 0.5475 - val_loss: 0.6568 - val_accuracy: 0.6902\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.5421 - accuracy: 0.7548 - val_loss: 0.4897 - val_accuracy: 0.8020\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4963 - accuracy: 0.7827 - val_loss: 0.4592 - val_accuracy: 0.8098\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3797 - accuracy: 0.8566 - val_loss: 0.4127 - val_accuracy: 0.8296\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2953 - accuracy: 0.8971 - val_loss: 0.3920 - val_accuracy: 0.8456\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.2512 - accuracy: 0.9166 - val_loss: 0.4088 - val_accuracy: 0.8500\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4693 - accuracy: 0.7309 - val_loss: 0.7041 - val_accuracy: 0.5094\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.6986 - accuracy: 0.5178 - val_loss: 0.6909 - val_accuracy: 0.5324\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 7s 12ms/step - loss: 0.6758 - accuracy: 0.5942 - val_loss: 0.7054 - val_accuracy: 0.5110\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.7063 - accuracy: 0.5085 - val_loss: 0.7031 - val_accuracy: 0.5160\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.6730 - accuracy: 0.5569 - val_loss: 0.4832 - val_accuracy: 0.8390\n",
            "Epoch 16: early stopping\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/GPU:0\"):\n",
        "    history = model.fit(train_ds, epochs=20, validation_data=val_ds, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLxXdoNVk8ku",
        "outputId": "16496688-16fb-4267-8f9e-ebd09df665ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 411ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 396ms/step\n"
          ]
        }
      ],
      "source": [
        "y_true = []\n",
        "y_pred_class = []\n",
        "\n",
        "for text_batch, label_batch in test_ds:\n",
        "    y_true.append(label_batch)\n",
        "    y_prob = model.predict(text_batch)\n",
        "    y_pred = tf.where(y_prob >= 0.5, 1, 0)\n",
        "    y_pred_class.append(tf.squeeze(y_pred))\n",
        "\n",
        "y_true = tf.concat(y_true, axis=0)\n",
        "y_pred_class = tf.concat(y_pred_class, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vP3vauumME6"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = tf.math.confusion_matrix(y_true, y_pred_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "INpHTO0-nx3Z",
        "outputId": "672066b2-eaa5-4333-8074-652a7d26c5c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Greens')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsFZukw1n1RY",
        "outputId": "a35545a0-6a06-4d3b-c14c-527919c77643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correctly classified reviews:\n",
            "This was the best film I saw in the year 2000. The Cohen brothers have never let me down before, and they certainly didn't this time either.<br /><br />It's one of those rare movies these days - it's witty, intelligent and vastly entertaining. I left the cinema with a warmth in my heart. Of course, there's lot of Cohen stuff in there - odd characters and peculiar gadgets, well-developed plot and magic camerawork. But no Cohen film is resembling any other Cohen film, if you overlook the general quality of them, of course.<br /><br />The big surprise for me was that Clooney is so good. But the true master performance in this movie comes from Tim Blake-Nelson. But the rest of the cast is superb too.<br /><br />A film that is lightweight comedy with a musical touch that evolve it's story round rednecks and old time country music - dripping with wit and intelligence. Thats a very unlikely combination. But it's exactly what this picture is.\n",
            "Predicted review: Positive\n",
            "\n",
            "Being an admitted chess addict, I was excited to see a documentary about the 1997 rematch between Garry Kasparov and IBM's Deep Blue supercomputer. I was hoping to see an in depth look at the match and a lot of what Kasparov had to say. Boy was I wrong and misguided by a mile. This documentary is a lot like many modern documentaries - there is a lot of flash but no real substance. After watching it, I am sad to say, I felt like I wasted my time. One of the most annoying aspects about the documentary is that it does not stay at one place for a decent period of time. It has the typical MTV type editing, where the camera shows different images and quick sound bites from people every five seconds. It is very sad that film-making has been watered down to the attention span of a 10-year old child.<br /><br />I understand it is difficult to make a film about chess, but that does not mean one should make it flashy. 'Game Over' did have a couple of interesting ideas though. It brought up the idea whether computers can think like human beings or not; whether computers have advanced to a unique new level. This is what Kasparov thought after the match, but this film does not go deep enough with this idea. Also, this film tries to bring in a bizarre theory. It tries to imply the paranoid that a human being was making the moves along with the help of the computer. Kasparov had suspicions about this, but still to this day there is no evidence. Towards the end of the film, it tries to imply the bizarre that maybe Anatoly Karpov might have been the human being who was secretly making the moves with the aid of Deep Blue. Interesting to think about, but I don't know how plausible or realistic it is. I still would not recommend this movie though, not even for chess addicts.\n",
            "Predicted review: Negative\n",
            "\n",
            "This movie was nothing like the book. <br /><br />Everything was mixed up or changed. Most of the movie was things that weren't even in the book.<br /><br />This movie never should have been viewed. It was a great disappointment to me when I enjoyed the book so much and then to watch how this movie trashed the entire thing.<br /><br />I would never recommend this movie to anyone that is a fan of Nora Roberts or J.D. Robb.<br /><br />Honestly this movie is not worth watching with how off from the book is really is.\n",
            "Predicted review: Negative\n",
            "\n",
            "This mini-series is iconic of the Australian spirit. While there may be what are perhaps considered glaring inconsistencies, the film portrays a spirit that is unique to Australia, and one that should be cherished. If anything, this mini-series demonstrates the Aussie sense of humour. The ability to laugh at the supremely ridiculous. Our willingness to have a laugh in even the most dire of situations. While a large part of this series focuses on mateship and how humour can be used as a means of survival, it also has elements of drama that are evoke real emotions. The main actors who appear are absolutely sensational and very convincing in their different roles. Capturing the essence of their characters perfectly. This mini-series should be mandatory viewing for all Australians.\n",
            "Predicted review: Positive\n",
            "\n",
            "George Barry is a genius. \"Death Bed: The Bed That Eats\" is a prototype for much of the 'slipstream' fiction and camp surrealism that is so chic now. Truly innovative, maverick, and just effing brilliant. Hyper-strange acting, subtly nightmarish atmosphere. I recommend reading Stephen Thrower's book \"Nightmare USA\" (there is a chapter devoted to Barry and \"Death Bed: The Bed That Eats\"). Available from FAB Press. On a related note, \"Death Bed: The Bed That Eats\" and \"Beyond Dream's Door\" make a perfect double-bill. Furthermore, it's trite and tired - and ultimately stupidly ironic - to criticize a low-budget cult film for being 'poorly made' or 'technically inept.' The B-movie aesthetic is part of these films' charm. No amount of CGI could duplicate the cumulative effect \"Death Bed: The Bed That Eats\" has on the viewer with an advanced palate.\n",
            "Predicted review: Positive\n",
            "\n",
            "I actually joined this site simply to write in about this movie. I was sitting in my living room and this movie came on one of the local channels. I made it about an hour through before I simply had enough. Curious to see what the general movie-opinionated public thought of this movie, I looked it up on this site. I was absolutely shocked to see that there were an overwhelming amount of people that thought it was great. I needed to have my say, and here it is: This movie is absolute garbage. It was a chore to sit through. The \"jokes\" were uninspired rehashes from other, better shows and movies, and Leguizamo's manic portrayal of this obnoxious character should only appeal to age ten and below. That actually may be a stretch even for that age. I'm all for slapstick ridiculousness, but there isn't even the faintest hint of wit or cleverness. I have an idea, lets take bad uninspired obvious jokes and play them at twice the speed. Now that's funny. Ha. Ha.<br /><br />Movies that you should see that take silly humor and add comic timing and originality: The Marx Brothers' A Night at the Opera, Monty Python's The Meaning of Life, South Park: Bigger, Longer, and Uncut,...and the list goes on. Don't lose an hour and a half of your life on unmemorable crap.<br /><br />By the way, I can only assume that the reason that David Bar Katz (the other writer) did VERY little in film after this movie is because he was instantly blacklisted. I'm actually impressed that Leguizamo was able to recover after this mess.\n",
            "Predicted review: Negative\n",
            "\n",
            "Incorrectly classified reviews:\n",
            "A tremendous action movie that I have ever seen. It was the first expression that I uttered after watching it twice on the local TV in my country, Indonesia. A combination between a constant shooting and a great fighting choreography played well by David Bradley. He was good here besides \"American Samurai\". His skill in martial art was performed above average. He succeeded to make the fighting scenes nice to watch. Not only punching and kicking like what most of martial artists show in their movies. David Bradley could utilize all the martial arts techniques such as throwing, smashing and kicking. Very Recommended for action fans.I have tried to look for the DVD but I still cannot get it particularly in my town. Anyone could help me?\n",
            "Predicted review: Negative\n",
            "\n",
            "I found this film completely and utterly incomprehensible. I knew some of he facts about Caravagggio, but here they were twisted and puzzling. The images were weirdly interesting but I was looking more for a biographical and/or critical accounting of Caravaggio's life and works, not an LSD type drug trip. The dialogue was very confusing and jumping back and forth in time via the use of trains, calculators, typewriters and cigarettes was extremely distracting. Had it been labelled an \"artsy film\" I wouldn't have purchased the DVD; now I have a DVD that I'll never watch again and who would buy it? I prefer mainstream films not those that require translation or elucidation. Thumbs down on this one for me!\n",
            "Predicted review: Positive\n",
            "\n",
            "OK. Who ever invented this film hates humanity and wants to see them all slit their throats. This \"film\" was absolute and utter filth. What the heck was up with the weird old bags eyes? Seriously, was she on some sort of horrible drug and then she like just thought she could control people? She was running around with her freaking evil eye and it was like what? Do I have a booger hanging out of my nose? What are you staring at? Are you like the sea witch or something? All and all though I thought the graphics were top notch old chap. For that alone I would give it a ten. But just cover your ears when you are watching it. The pure and complete evil that comes from that film will make your ears bleed and your eyelids fall off. Who knows? You might even get a knot in your small intestine. You better watch out fools.\n",
            "Predicted review: Negative\n",
            "\n",
            "I think the manuscript of this movie was written on the piece of toilet-paper. No respect whatsoever to many important details which intrinsically make the movie. For example, the names of some Serbian terrorists (that I remember) are Caradan Maldic, Ivanic Loyvek and Leo Hasse. What kind of names are that? Certainly not Serbian! By the way, Caradan Maldic!!! What a name, I laughed for days thinking about it. Probably an implication on Karadzic and Mladic. Secondly, there have never been any cases of terrorism done by Serbians. A journalist like the main character ought to have known that. Thirdly, the actors playing Serbian terrorists are not even Serbs nor do they speak Serbo-croatian. All this aside, this movie is solidly acted but the story is paper-thin and full of holes. At times it makes no sense whatsoever!!!\n",
            "Predicted review: Negative\n",
            "\n",
            "So you have the spoiler warning---but I would argue that you cannot spoil what is already rotten. I assume they changed the name to \"The Cavern\" just in case \"WIthIn\"s reputation had preceded it.<br /><br />After paying the cable rental for this movie, I considered saving my household garbage for a month and mailing it to the writer/director. He had his garbage delivered to my home, so I thought it only fair that I return the favor.<br /><br />The movie opens with a suggestion that the scene is in the desert of Kazakhstan. I'm not sure why they picked Kazakhstan; maybe the writer is a fan of the Ali G Show. But they should have just started inside the cave, because the outside was obviously not Kazakhstan. It was the first clue that I was going to hate the movie.<br /><br />The movie has no redeeming qualities, save one: it's consistent. Everything is terrible. The writing, the directing, the acting, the cinematography---every aspect of this film is just bad. And I like bad films, goofy films, B-horror films . . . but this was just plain bad. And stupid. And hackneyed. And predictable. And boring.<br /><br />To get a feel for the film, go into your laundry room with 5 of your friends, and turn off the lights. Put a flashlight (turned on) into your dryer and start it tumbling. Now all of you start screaming and yelling at the top of your lungs. That's it.<br /><br />For a complete re-enactment, have 5 of the 6 people in the laundry room play dead on the floor. Toss Karo syrup on them. Turn the lights back on (stop the dryer). Now have a guy in a gorilla costume enter the room and rape the last person standing.<br /><br />FIN<br /><br />ADDENDUM: Reading through the other comments, many find it remarkable this movie was made on a low budget. That's not remarkable. Making a crap movie on a HUGE budget is remarkable (Waterworld). Making a good movie on a low budget is remarkable (like Blair Witch, which I thoroughly enjoyed). Making a crap movie on a low budget isn't a bit surprising, and you can expect more of the same if these people are still making movies, because I can't imagine anybody would hand them a pile of cash after watching this.<br /><br />Is the low budget an excuse for a terrible film? No, and it's certainly no reason to watch it. Would you eat a dog-dung sandwich just because it was cheap to make?<br /><br />The IMDb rating for this film over time will be interesting to watch. It should trend farther downward, but only if the number of unsuspecting innocent viewers can outpace the movie makers' ability to beg their personal friends to give it 10 stars.\n",
            "Predicted review: Positive\n",
            "\n",
            "i enjoyed this film immensely, due to pungent scenes (humorous as well as ironic, some even \"tragical\"), believable performances, witty dialogue and a heartfelt rendering of what it´s like or rather c a n be like to be hetero- and/or homosexual & on the lookout for fulfilment of your desires. i´m aware of the paradox here: homo- a n d hetereosexual.... this is something the film tackles on end, but never uses for caricature. if you´re as open-minded as the people seem to have been who made that film, in the end it won´t matter to you if those who lie in each others arms are of the same sex or not.<br /><br />\"mr. smith\" from the matrix gives an admirable turn as a gay houses-salesman with \"strange\" appetites here, but that´s not the only thing to marvel at. enjoy.....\n",
            "Predicted review: Positive\n",
            "\n"
          ]
        }
      ],
      "source": [
        "correct = []\n",
        "for idx in range(len(y_true)):\n",
        "    true_label = y_true[idx]\n",
        "    pred_label = y_pred_class[idx]\n",
        "    if true_label == pred_label:\n",
        "        correct.append(idx)\n",
        "\n",
        "incorrect = []\n",
        "for idx in range(len(y_true)):\n",
        "    true_label = y_true[idx]\n",
        "    pred_label = y_pred_class[idx]\n",
        "    if true_label != pred_label:\n",
        "        incorrect.append(idx)\n",
        "\n",
        "num_examples = 6\n",
        "\n",
        "print(\"Correctly classified reviews:\")\n",
        "count = 0\n",
        "for idx, (text_batch, label_batch) in enumerate(raw_test_ds.unbatch().batch(1)):\n",
        "    if count >= num_examples:\n",
        "        break\n",
        "    if idx in correct:\n",
        "        count += 1\n",
        "        text = text_batch[0].numpy().decode('utf-8')\n",
        "        if tf.equal(y_pred_class[idx], 0):\n",
        "          review = \"Negative\"\n",
        "        else:\n",
        "          review = \"Positive\"\n",
        "        print(f\"{text}\\nPredicted review: {review}\\n\")\n",
        "\n",
        "print(\"Incorrectly classified reviews:\")\n",
        "count = 0\n",
        "for idx, (text_batch, label_batch) in enumerate(raw_test_ds.unbatch().batch(1)):\n",
        "    if count >= num_examples:\n",
        "        break\n",
        "    if idx in incorrect:\n",
        "        count += 1\n",
        "        text = text_batch[0].numpy().decode('utf-8')\n",
        "        if tf.equal(y_pred_class[idx], 0):\n",
        "          review = \"Negative\"\n",
        "        else:\n",
        "          review = \"Positive\"\n",
        "        print(f\"{text}\\nPredicted review: {review}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBqoP_r30gtl"
      },
      "source": [
        "I constructed a text classification workflow from scratch involving tokenization, building a CNN-LSTM model, specifying the loss function, feeding train/validation data to Model.fit, plotting the confusion matrix, and listing several correctly and incorrectly classified reviews.\n",
        "\n",
        "I also implemented custom standardization of the text, used pre-trained GloVe embeddings for word representations, and applied regularization techniques like L2 regularization and dropout to prevent overfitting.\n",
        "\n",
        "Regularization techniques like L2 regularization and dropout were used to prevent overfitting and improve the generalization performance of the model. L2 regularization penalizes large weight values, while dropout randomly drops out a percentage of the neurons during training to reduce co-adaptation.\n",
        "\n",
        "The model used a combination of convolutional and recurrent layers to capture the spatial and temporal dependencies in the text data. The model included an embedding layer to map the text data to low-dimensional vectors, a convolutional layer to extract local features, a pooling layer to reduce the dimensionality of the feature maps, an LSTM layer to capture the long-term dependencies in the text, and a dense layer to output the predicted sentiment label.\n",
        "\n",
        "I was able to achieve the accuracy of upto 85%, indicating that it was able to successfully classify movie reviews as positive or negative. I tried additional layer, hyper-parameter tuning and other optimisation, but however accuracy hovered around 85% and lower. I also tried to using Word2Vec for pre-trained word embeddings. I tried Data augumentation but IMBD dataset is already quite large and diverse.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd41fsibLlzy"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://keras.io/examples/nlp/text_classification_from_scratch/\n",
        "\n",
        "https://keras.io/examples/nlp/pretrained_word_embeddings/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSnjKA6G1buq"
      },
      "source": [
        "**seq2seq model:**\n",
        "\n",
        "Seq2seq is a family of machine-learning approaches used for natural language processing. Applications include language translation, image captioning, conversational models, and text summarization. \"ted_multi_translate\" is a multilingual (60 language) data set derived from TED Talk transcripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQnAtkkW90PB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, Dropout, LSTM, Embedding, Attention, Dot\n",
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrZR2Fv8tUky",
        "outputId": "85fb699f-0c97-4779-ec7a-bfe85d3a365c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDpX90qUvmF4"
      },
      "outputs": [],
      "source": [
        "#dataset downloaded from https://github.com/neulab/word-embeddings-for-nmt\n",
        "#dataset has been read and processed for English and Roman texts using the provided py script\n",
        "\n",
        "en_path = '/content/drive/MyDrive/ML_assign_4/data/en_ro/train.en'\n",
        "ro_path = '/content/drive/MyDrive/ML_assign_4/data/en_ro/train.ro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83sJYoX4twHW"
      },
      "outputs": [],
      "source": [
        "train_en = []\n",
        "train_ro = []\n",
        "with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        train_en.append(line.strip())\n",
        "with open(ro_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        train_ro.append(line.strip())\n",
        "\n",
        "train_df = pd.DataFrame({\"en\": train_en, \"ro\": train_ro})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlGf1VdHxT1A"
      },
      "outputs": [],
      "source": [
        "en_path_test = '/content/drive/MyDrive/ML_assign_4/data/en_ro/test.en'\n",
        "ro_path_test = '/content/drive/MyDrive/ML_assign_4/data/en_ro/test.ro'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_qljn0MxavY"
      },
      "outputs": [],
      "source": [
        "test_en = []\n",
        "test_ro = []\n",
        "with open(en_path_test, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        test_en.append(line.strip())\n",
        "with open(ro_path_test, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        test_ro.append(line.strip())\n",
        "\n",
        "test_df = pd.DataFrame({\"en\": test_en, \"ro\": test_ro})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzqV7BFJzO-K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUcP4SJoNrbl"
      },
      "outputs": [],
      "source": [
        "max_len = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9b9BVqENgh4"
      },
      "outputs": [],
      "source": [
        "#Ref: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "tokenizer_en = Tokenizer()\n",
        "tokenizer_ro = Tokenizer()\n",
        "tokenizer_en.fit_on_texts(train_en)\n",
        "tokenizer_ro.fit_on_texts(train_ro)\n",
        "#learn vocabulary based on the in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbYn6llGQwNq"
      },
      "outputs": [],
      "source": [
        "vocab_size_en = len(tokenizer_en.word_index) + 1\n",
        "vocab_size_ro = len(tokenizer_ro.word_index) + 1\n",
        "#add 1 dor padding token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgf-ZLuUQ6jT"
      },
      "outputs": [],
      "source": [
        "train_en_seq = tokenizer_en.texts_to_sequences(train_en)\n",
        "train_ro_seq = tokenizer_ro.texts_to_sequences(train_ro)\n",
        "\n",
        "test_en_seq = tokenizer_en.texts_to_sequences(test_en)\n",
        "test_ro_seq = tokenizer_ro.texts_to_sequences(test_ro)\n",
        "#convert data for both languages into sequences of integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcdwNWJfRDLP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_en_padded = pad_sequences(train_en_seq, maxlen=max_len, padding=\"post\")\n",
        "train_ro_padded = pad_sequences(train_ro_seq, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "test_en_padded = pad_sequences(test_en_seq, maxlen=max_len, padding=\"post\")\n",
        "test_ro_padded = pad_sequences(test_ro_seq, maxlen=max_len, padding=\"post\")\n",
        "#padding to make inputs same length for NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOwQoXcza9K4",
        "outputId": "0fe7cad7-d999-464a-c85c-92a08f43749d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Detected 400000 Word Vectors***\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import io\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "def getEmbeddings(path):\n",
        "    emb_idx = {}\n",
        "    with zipfile.ZipFile(path, \"r\") as f:\n",
        "        with io.TextIOWrapper(f.open(\"glove.6B.100d.txt\"), encoding=\"utf-8\") as text_file:\n",
        "            for line in text_file:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                float_values = []\n",
        "                for val in values[1:]:\n",
        "                    float_val = float(val)\n",
        "                    float_values.append(float_val)\n",
        "\n",
        "                coefs = tf.constant(float_values)\n",
        "                emb_idx[word] = coefs\n",
        "    return emb_idx\n",
        "\n",
        "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "path = \"glove.6B.zip\"\n",
        "urllib.request.urlretrieve(url, path)\n",
        "embs_idx = getEmbeddings(path)\n",
        "print(\"***Detected %s Word Vectors***\" % len(embs_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx48_xwsFFzv"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer_en.word_index\n",
        "num_tokens = len(word_index) + 1\n",
        "embedding_dim = len(next(iter(embs_idx.values())))\n",
        "embedding_matrix = tf.Variable(tf.zeros((num_tokens, embedding_dim)))\n",
        "#getting embedding matrix from tokenizer_en with emb_idx (from glove)\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embs_idx.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix = tf.tensor_scatter_nd_update(embedding_matrix, [[i]], [embedding_vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sCBB9LzNud1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "#embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNak-r-LcuXB"
      },
      "outputs": [],
      "source": [
        "#Encoder\n",
        "encoder_inputs = Input(shape=(max_len,))\n",
        "encoder_emb = Embedding(vocab_size_en, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, hidden_state, cell_state = encoder_lstm(encoder_emb)\n",
        "encoder_states = [hidden_state, cell_state]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NzvBQz5CoBW"
      },
      "outputs": [],
      "source": [
        "word_index = tokenizer_ro.word_index\n",
        "num_tokens = len(word_index) + 1\n",
        "embedding_dim = len(next(iter(embs_idx.values())))\n",
        "embedding_matrix_ro = tf.Variable(tf.zeros((num_tokens, embedding_dim)))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embs_idx.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_ro = tf.tensor_scatter_nd_update(embedding_matrix_ro, [[i]], [embedding_vector])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrljaZFscv-k"
      },
      "outputs": [],
      "source": [
        "#Decoder\n",
        "decoder_inputs = Input(shape=(max_len,))\n",
        "decoder_emb_layer = Embedding(vocab_size_ro, 100, weights=[embedding_matrix_ro], input_length=max_len, trainable=False)\n",
        "decoder_emb = decoder_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_hidden_state, decoder_cell_state = decoder_lstm(decoder_emb, initial_state=encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_hR0_eccyC7"
      },
      "outputs": [],
      "source": [
        "#Attention layer\n",
        "attention = Attention()\n",
        "context_vector = attention([decoder_outputs, encoder_outputs])\n",
        "decoder_concat = Concatenate(axis=-1)([decoder_outputs, context_vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-pnlQxUm30P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1e53bd-e107-4d92-ee10-0e306b2b9357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 500)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 500, 100)     4853300     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 500, 100)     9784900     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 500, 1024),  4608000     ['embedding[0][0]']              \n",
            "                                 (None, 1024),                                                    \n",
            "                                 (None, 1024)]                                                    \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 500, 1024),  4608000     ['embedding_1[0][0]',            \n",
            "                                 (None, 1024),                    'lstm[0][1]',                   \n",
            "                                 (None, 1024)]                    'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, 500, 1024)    0           ['lstm_1[0][0]',                 \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 500, 2048)    0           ['lstm_1[0][0]',                 \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 500, 97849)   200492601   ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 224,346,801\n",
            "Trainable params: 209,708,601\n",
            "Non-trainable params: 14,638,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Dense layer\n",
        "decoder_dense = Dense(vocab_size_ro, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_concat)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKrvWpzrN4-9"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "epochs = 10\n",
        "batch_size = 4\n",
        "\n",
        "history = model.fit([train_en_padded, train_ro_padded], train_ro_padded, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGU7j9LMOBA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3e6d846-3eaf-41ad-d5af-c9b02189a501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1158/1158 [==============================] - 604s 521ms/step - loss: 0.1224 - accuracy: 0.9864\n",
            "Test Loss: 0.12242291867733002\n",
            "Test Accuracy: 0.9864163398742676\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate([test_en_padded, test_ro_padded], test_ro_padded, batch_size=batch_size)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsEOZ-Pe4K88"
      },
      "source": [
        "I build the Seq2Sqe from scratch for Ted Text translation which achieved test accuracy nearly 96% when tested with partial epochs. In this sequence-to-sequence machine translation task, I used the GloVe word embeddings to initialize the embedding layer of our model. The encoder and decoder both used LSTM layers with attention mechanism to capture the context of the input sentence and generate the corresponding translation. The model was trained using teacher forcing, where the ground-truth output sequence was fed as input to the decoder at each time step.\n",
        "\n",
        "However, training the model with such a large number of parameters can result in high memory usage and slow training time. In fact, I **faced issues with memory usage during training**, even with a **GPU with 15GB of VRAM and 12.7GB of RAM**. I was able to make it work after optimising batch size. Therefore, it may be necessary to consider techniques like gradient checkpointing or model parallelism to reduce the memory footprint of the model and accelerate training.\n",
        "\n",
        "To improve the performance of the model, I worked with experimenting different hyperparameters, such as the number of LSTM units, to find the optimal configuration. Additionally, alternative pre-trained embeddings like BERT could be used to further improve the representation of the input and output sequences."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZyGf/WR8HwPk1gWyX+c71",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}